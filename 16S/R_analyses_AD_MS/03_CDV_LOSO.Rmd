---
title: "CDV LOSO"
author: "SRomano"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Load packages

```{r setup}
library(phyloseq)
library(dplyr)
library(mlr3)
library(SIAMCAT)
require(mlr3extralearners)
library(ggplot2)

source("/Scripts/Cross_study_validation.r")
source("/Scripts/Siamcat_wf.r")

set.seed(56987)
```

# Re-define functions
Define functions to use with the LOSO models
```{r}
set.seed(56987)

cross.study.validation<-function(list.models,
                                 microbiome,
                                 meta.t,
                                 names.studies,
                                 label="PD", 
                                 case="PD",
                                 auc.plot = F,
                                 name.study.var = "Study",
                                 study.in.models = T,
                                 meta.test.hold = NULL,
                                 microbiome.test.hold = NULL,
                                 name.study.var.hold = NULL,
                                 label.hold = NULL,
                                 case.hold = NULL,
                                 direction = "<"){
      std<-names(list.models@siamcat.train) 
      # list to store the models
      l_csv<-vector(mode = "list", length = length(std))
      names(l_csv)<-std
      
      for(i in 1:length(std)){
        print(paste("The LOSO model to test is missing", std[i]))
        # get the name of the first model
        train<-std[i]
        # define the test data to use for this model
        test.data<-names.studies
        # select the list of models to test
        l_train<-list.models@siamcat.train[[i]] # this is the model trained on the original data
        # create a list were to store the testing data
        l_all_tests<-vector(mode = "list", length = length(test.data))
        # run cross study validation
        for(tst in 1:length(test.data)){
          print(paste("Testing the model on", test.data[tst]))
          test.study<-test.data[tst]
          # select data that need to be tested
          meta.test<-meta.test.hold[meta.test.hold[, name.study.var.hold] == test.study,] # these are other meta for the testing data
          microbiome.test<-microbiome.test.hold[, meta.test$SampleID] # these are the microbiome for the testing data
          stopifnot(all(meta.test$SampleID == names(microbiome.test)))
          ############################# 
          # SIAMCAT workflow for testing the train model on the hold out data sets
          #############################
          # create object
          sc.obj.test <- siamcat(feat=microbiome.test, meta=meta.test,label=label.hold, case=case.hold)
          # make prediction with each model in the list of training data
          prediction <- make.predictions(l_train, siamcat.holdout = sc.obj.test)
          # uses this to calculate auc using as a reference the original meta data of the testing set
          roc<-evaluate.predictions(prediction) 
          
          # prepare a summary vector containing the details of the tests
          out<-c(train, test.study, as.double(roc@eval_data$auroc))
          names(out)<-c("training.data", "testing.data", "AUC")
          
          l_test<-list(out, prediction)
          names(l_test)<-c("summary", "testing.siamcat")
          # the results are appended in a list of lists
          # level 1 - names of the trained models (in this case the loso built leaving out X dataset)
          # level 2 - summary vector + the the testing siam obj
          print("Appending the results list to final list")
          l_all_tests[[tst]]<-l_test
          print("Rename list")
          names(l_all_tests)[[tst]]<-paste0(std[i], "_vs_", test.study)
        }
        l_csv[[i]]<-l_all_tests
      }
      return(l_csv)
}




cross.disease.pred<-function(list.of.csv.models,
                             list.train.models,
                             prob.test.train = c("train", "left-out")) {
  require(SIAMCAT)
  require(pROC)
  # do some check if they are not in the same order.
  # Check here the names
    train.models<-names(list.of.csv.models)
    test<-names(list.train.models@siamcat.train)
    stopifnot(all(train.models==test))
    # double check all orders are the same
    stopifnot(all(names(list.train.models) == train.models))

    # create an object to store the results
    empty<-c()
    
    # take the train model and make a prediction. 
    if(prob.test.train == "train") {
      print("I will use the LOSO model and re-evaluate prediction on the data")
      list.train.models<-list.train.models@siamcat.train
      } else if(prob.test.train == "left-out") {
        print("I will use the left out data and the respective predictions")
        list.train.models<-list.train.models@siamca.test
      } else {message("The reference models to use is either the LOSO or the left out study in the LOSO")}
    
    for(i in 1:length(train.models)){
      training.data<-train.models[i]
          if(prob.test.train == "train"){
            list.train.models[[i]]<-make.predictions(list.train.models[[i]])
            list.train.models[[i]]<-evaluate.predictions(list.train.models[[i]])
          }
      # extract the roc for each model
      t.auc<-eval_data(list.train.models[[i]])$roc
      size.train<-list.train.models[[i]]@label$label %>% length()
      train.auc<-t.auc$auc %>% as.double()
      sp<-t.auc$specificities # here we extract the specificity from the auc predictions
      
      m<-which.min(abs(sp - 0.9)) 
      fpr.value<-abs(1-sp)[m] # get the fpr value for that we are actually selecting, as a double check
      # extract the threshold that correspond to 10% FPR
      tr<-t.auc$thresholds[m]
      threshold.train<-tr
      # now use this threshold to see how many samples were predicted above it in the testing data set of the other disease
   
      mod<-list.of.csv.models[[i]]
      for(t in 1:length(mod)){
        train_vs_test<-names(mod)[t]
        testing.data<-gsub(".*_vs_", "", names(mod)[t])
        pro.test<-pred_matrix(mod[[t]]$testing.siamcat) 
        # the above gives me a prediction probability for all samples for all 10 repetition of the 10 models in the CV.
        # make a df and calculate the average
        prot.average<-apply(pro.test, 1, mean)
        x<-prot.average[prot.average >= tr] # select all probabilities >= to the 10% FPR threshold
        # calculate prediction rates (ration of samples > threshold and total sample in testing data)
        pr<-(length(x)/length(prot.average))*100 # prob vector contains all values for all samples in the testing set. 
        
        tmp<-c(training.data, 
               size.train,
               testing.data,
               train.auc,
               train_vs_test,
               fpr.value,
               m, 
               threshold.train,
               pr)
        empty<-rbind.data.frame(empty, tmp)
      }
    }
    names(empty)<-c("training.data",
                    "size.train",
                    "testing.data" ,
                    "train.auc",
                    "train_vs_test",
                    "fpr.value",
                    "index.fpr",
                    "threshold.train",
                    "perc.pred")
    return(empty)
}

```

# Load LOSO models
```{r}
r_5x10<-readRDS("/16S/R_analyses/RDS/16S_loso.ridge.rds")
```

# Load data

```{r}
# load other diseases
all.tss<-readRDS("16S/01_16S_databases/AD_MS_GTDB2017/RDS/all_g_TSS.rds")
all.tss.store<-all.tss # add that this for consistency
meta<-readRDS( "16S/R_analyses_AD/RDS/meta_ad.rds")
meta$PD<-as.character(meta$PD)
# specify that not diseases are HC and other diseases are disease
meta[meta$PD != "HC",]$PD<- "other-diseases"

#CDV
# remove duplicates
dup<-c("SRR8534161","SRR8534166","SRR8534186")
dim(meta)
meta<-meta[-1*which(meta$SampleID %in% dup),]
dim(meta)
meta %>%
  group_by(Study) %>%
  summarise(n = n())
```


# Filter
```{r}
all.tss<-all.tss[, meta$SampleID]

idx.r<-which(rowSums(all.tss) == 0)
idx.c<-which(colSums(all.tss) == 0)
idx.c
all.tss<-all.tss[-idx.r, ]

meta<-meta[order(meta$SampleID),]
all.tss<-all.tss[,order(names(all.tss))]

stopifnot(all(names(all.tss) == meta$SampleID))
```

# Run models
```{r}
# run loso csv
cdv_r5x10<-cross.study.validation(list.models = r_5x10, 
                                  microbiome = NULL,
                                 meta.t = NULL,
                                 names.studies = unique(meta$Study),
                                 label="PD", 
                                 case="PD",
                                 auc.plot = F,
                                 name.study.var = NULL, 
                                 study.in.models = F,
                                 meta.test.hold = meta,
                                 microbiome.test.hold = all.tss,
                                 name.study.var.hold = "Study",
                                 label.hold = "PD",
                                 case.hold = "other-diseases",
                                 direction = "<")
saveRDS(cdv_r5x10, "/16S/R_analyses_AD/RDS/LOSO_cdv_r5x10.rds")


## Use the model for the LOSO training
df_ridge<-cross.disease.pred(list.of.csv.models = cdv_r5x10, 
                             list.train.models =  r_5x10,
                             prob.test.train = "left-out")


df_ridge$prev<-rep("5% in 10", nrow(df_ridge))
df_ridge$ML<-rep("ridge", nrow(df_ridge))
df_ridge$LOSO<-rep("LOSO.LO", nrow(df_ridge))
saveRDS(df_ridge, "/16S/R_analyses_AD/RDS/LOSO_CDV.rds")

```
